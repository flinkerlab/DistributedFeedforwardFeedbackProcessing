{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set()\n",
    "#sns.set_style('whitegrid')\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('/scratch/xc1490/projects/tmp/python_packages')\n",
    "sys.path.append('/scratch/xc1490/projects/tmp/python_packages/') #pip install --target=/home/xc1490/home/projects/tmp/python_packages package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/xc1490/projects/ecog/ALAE_1023/code_release\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/xc1490/projects/ecog/ALAE_1023/code_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formant_freq_limits_diff = torch.tensor([950.,2450.,2100.]).reshape([1,3,1]) #freq difference\n",
    "formant_freq_limits_diff_low = torch.tensor([300.,300.,0.]).reshape([1,3,1]) #freq difference\n",
    "formant_freq_limits_abs = torch.tensor([950.,3400.,3800.,5000.,6000.,7000.]).reshape([1,6,1]) #freq difference\n",
    "formant_freq_limits_abs_low = torch.tensor([300.,700.,1800.,3400,5000.,6000.]).reshape([1,6,1]) #freq difference\n",
    "formant_freq_limits_abs_noise = torch.tensor([8000.,7000.,7000.]).reshape([1,3,1]) #freq difference\n",
    "formant_freq_limits_abs_noise_low = torch.tensor([4000.,3000.,3000.]).reshape([1,3,1]) #freq difference\n",
    "formant_bandwitdh_ratio = Parameter(torch.Tensor(1))\n",
    "formant_bandwitdh_slop = Parameter(torch.Tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_Para_Prediction(nn.Module):\n",
    "    def __init__(self, causal, anticausal, compute_db_loudness=False, n_formants= 6,n_formants_noise=1,\\\n",
    "                 n_mels=32,network_db=False,input_channel=32):\n",
    "        super(Speech_Para_Prediction, self).__init__()\n",
    "        norm = GroupNormXDim\n",
    "        \n",
    "        self.n_formants = n_formants\n",
    "        self.n_mels = n_mels\n",
    "        self.n_formants_noise = n_formants_noise\n",
    "        self.compute_db_loudness = compute_db_loudness\n",
    "        self.network_db = network_db\n",
    "        self.formant_freq_limits_diff = formant_freq_limits_diff\n",
    "        self.formant_freq_limits_diff_low = formant_freq_limits_diff_low\n",
    "        self.formant_freq_limits_abs = formant_freq_limits_abs\n",
    "        self.formant_freq_limits_abs_low = formant_freq_limits_abs_low\n",
    "        self.formant_freq_limits_abs_noise = formant_freq_limits_abs_noise\n",
    "        self.formant_freq_limits_abs_noise_low = formant_freq_limits_abs_noise_low\n",
    "        self.formant_bandwitdh_ratio = formant_bandwitdh_ratio\n",
    "        self.formant_bandwitdh_slop = formant_bandwitdh_slop\n",
    "        with torch.no_grad():\n",
    "            nn.init.constant_(self.formant_bandwitdh_ratio,0)\n",
    "            nn.init.constant_(self.formant_bandwitdh_slop,0)\n",
    "        \n",
    "        self.conv_fundementals = ln_c.Conv1d(input_channel,32,3,1,1 ,causal=causal,anticausal = anticausal)\n",
    "        self.norm_fundementals = norm(32,32)\n",
    "        self.f0_drop = nn.Dropout()\n",
    "        self.conv_f0 = ln_c.Conv1d(32,1,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_amplitudes = ln_c.Conv1d(input_channel,2,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_amplitudes_h = ln_c.Conv1d(input_channel,2,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        if compute_db_loudness:\n",
    "            self.conv_loudness = ln_c.Conv1d(input_channel,1,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        else:\n",
    "            self.conv_loudness = ln_c.Conv1d(input_channel,1,1,1,0,bias_initial=-9.  ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants = ln_c.Conv1d(input_channel,32,3,1,1 ,causal=causal,anticausal = anticausal)\n",
    "        self.norm_formants = norm(32,32)\n",
    "        self.conv_formants_freqs = ln_c.Conv1d(32,n_formants,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_bandwidth = ln_c.Conv1d(32,n_formants,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_amplitude = ln_c.Conv1d(32,n_formants,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_freqs_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_bandwidth_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_amplitude_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0 ,causal=causal,anticausal = anticausal)\n",
    "    def forward(self, x_common):\n",
    "        if self.compute_db_loudness:\n",
    "            loudness = F.sigmoid(self.conv_loudness(x_common)) #0-1\n",
    "            loudness = loudness*200-100 #-100 ~ 100 db\n",
    "            loudness = 10**(loudness/10.) #amplitude\n",
    "        else:\n",
    "            loudness = F.softplus(self.conv_loudness(x_common))\n",
    "        logits = self.conv_amplitudes(x_common)\n",
    "        amplitudes_logsoftmax = F.log_softmax(logits,dim=1)\n",
    "        amplitudes_h = F.softmax(self.conv_amplitudes_h(x_common),dim=1)\n",
    "        x_fundementals = self.f0_drop(F.leaky_relu(self.norm_fundementals(self.conv_fundementals(x_common)),0.2))\n",
    "        f0_hz = torch.sigmoid(self.conv_f0(x_fundementals)) * 332 + 88 # 88hz < f0 < 420 hz\n",
    "        f0 = torch.clamp(mel_scale(self.n_mels,f0_hz)/(self.n_mels*1.0),min=0.0001)\n",
    "\n",
    "        x_formants = F.leaky_relu(self.norm_formants(self.conv_formants(x_common)),0.2)\n",
    "        formants_freqs = torch.sigmoid(self.conv_formants_freqs(x_formants))\n",
    "        formants_freqs_hz = formants_freqs*(self.formant_freq_limits_abs[:,:self.n_formants]-self.formant_freq_limits_abs_low[:,:self.n_formants])+self.formant_freq_limits_abs_low[:,:self.n_formants]\n",
    "        formants_freqs = torch.clamp(mel_scale(self.n_mels,formants_freqs_hz)/(self.n_mels*1.0),min=0)\n",
    "        formants_bandwidth_hz = 0.65*(0.00625*torch.relu(formants_freqs_hz)+375)\n",
    "        formants_bandwidth = bandwidth_mel(formants_freqs_hz,formants_bandwidth_hz,self.n_mels)\n",
    "        formants_amplitude_logit = self.conv_formants_amplitude(x_formants)\n",
    "        formants_freqs_noise = torch.sigmoid(self.conv_formants_freqs_noise(x_formants))\n",
    "        formants_freqs_hz_noise = formants_freqs_noise*(self.formant_freq_limits_abs_noise[:,:self.n_formants_noise]-self.formant_freq_limits_abs_noise_low[:,:self.n_formants_noise])+self.formant_freq_limits_abs_noise_low[:,:self.n_formants_noise]\n",
    "        formants_freqs_hz_noise = torch.cat([formants_freqs_hz,formants_freqs_hz_noise],dim=1)\n",
    "        formants_freqs_noise = torch.clamp(mel_scale(self.n_mels,formants_freqs_hz_noise)/(self.n_mels*1.0),min=0)\n",
    "        formants_bandwidth_hz_noise = self.conv_formants_bandwidth_noise(x_formants)\n",
    "        formants_bandwidth_hz_noise_1 = F.softplus(formants_bandwidth_hz_noise[:,:1]) * 2344 + 586 #2000-10000\n",
    "        formants_bandwidth_hz_noise_2 = torch.sigmoid(formants_bandwidth_hz_noise[:,1:]) * 586 #0-2000\n",
    "        formants_bandwidth_hz_noise = torch.cat([formants_bandwidth_hz_noise_1,formants_bandwidth_hz_noise_2],dim=1)\n",
    "        formants_bandwidth_hz_noise = torch.cat([formants_bandwidth_hz,formants_bandwidth_hz_noise],dim=1)\n",
    "        formants_bandwidth_noise = bandwidth_mel(formants_freqs_hz_noise,formants_bandwidth_hz_noise,self.n_mels)\n",
    "        formants_amplitude_noise_logit = self.conv_formants_amplitude_noise(x_formants)\n",
    "        formants_amplitude_noise_logit = torch.cat([formants_amplitude_logit,formants_amplitude_noise_logit],dim=1)\n",
    "        if self.network_db:\n",
    "            formants_amplitude = torch.sigmoid(formants_amplitude_logit )    \n",
    "            formants_amplitude_noise = torch.sigmoid(formants_amplitude_noise_logit )\n",
    "            amplitudes = torch.sigmoid(logits ) \n",
    "            amplitudes = db_to_amp(amplitudes)\n",
    "            formants_amplitude_noise = db_to_amp(formants_amplitude_noise)\n",
    "            formants_amplitude = db_to_amp(formants_amplitude)\n",
    "            amplitudes = amplitudes/torch.sum(amplitudes,dim=1,keepdim=True)\n",
    "            formants_amplitude_noise = formants_amplitude_noise/torch.sum(formants_amplitude_noise,dim=1,keepdim=True)\n",
    "            formants_amplitude = formants_amplitude/torch.sum(formants_amplitude,dim=1,keepdim=True)\n",
    "        else:\n",
    "            formants_amplitude = F.softmax(formants_amplitude_logit,dim=1)    \n",
    "            formants_amplitude_noise = F.softmax(formants_amplitude_noise_logit,dim=1)\n",
    "            amplitudes = F.softmax(logits,dim=1) \n",
    "            \n",
    "        components = { 'f0':f0 ,\n",
    "                    'f0_hz':f0_hz ,\n",
    "                    'loudness':loudness ,\n",
    "                    'amplitudes':amplitudes ,\n",
    "                    'amplitudes_logsoftmax':amplitudes_logsoftmax ,\n",
    "                    'amplitudes_h':amplitudes_h ,\n",
    "                    'freq_formants_hamon':formants_freqs,\n",
    "                    'bandwidth_formants_hamon':formants_bandwidth ,\n",
    "                    'freq_formants_hamon_hz':formants_freqs_hz ,\n",
    "                    'bandwidth_formants_hamon_hz':formants_bandwidth_hz ,\n",
    "                    'amplitude_formants_hamon':formants_amplitude ,\n",
    "                    'freq_formants_noise':formants_freqs_noise ,\n",
    "                    'bandwidth_formants_noise':formants_bandwidth_noise ,\n",
    "                    'freq_formants_noise_hz':formants_freqs_hz_noise ,\n",
    "                    'bandwidth_formants_noise_hz':formants_bandwidth_hz_noise ,\n",
    "                    'amplitude_formants_noise':formants_amplitude_noise ,\n",
    "        }\n",
    "        return components\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECoGMapping_Bottleneck_ran(nn.Module):\n",
    "    def __init__(self,n_mels,n_formants,n_formants_noise=1,compute_db_loudness=False,causal=False, \\\n",
    "                     anticausal = False,pre_articulate=False,upsample='ConvTranspose',GR = False, \\\n",
    "                 network_db=False, latent_channel=32 ):\n",
    "        super(ECoGMapping_Bottleneck_ran, self).__init__()\n",
    "        self.causal = causal\n",
    "        self.anticausal = anticausal\n",
    "        causal_ = [1,0,0] if causal else False\n",
    "        self.pre_articulate = pre_articulate\n",
    "        norm = GroupNormXDim\n",
    "        base = 32 if self.pre_articulate else 16\n",
    "        conv1_in_channel =  base\n",
    "        self.from_ecog = FromECoG_causal(base,in_channel=1,residual=True,causal=causal_,anticausal = anticausal) \n",
    "        self.conv1 = ECoGMappingBlock_causal(conv1_in_channel,base*2,[5,1,1],residual=True,resample = [2,1,1],pool='MAX',causal=causal_,anticausal = anticausal) \n",
    "        self.conv2 = ECoGMappingBlock_causal(base*2,base*4,[3,1,1],residual=True,resample = [(1 if self.pre_articulate else 2),1,1],pool='MAX',causal=causal_,anticausal = anticausal) \n",
    "        self.conv3 = ECoGMappingBlock_causal(base*4,base*8,[3,3,3],residual=True,resample = [2,2,2],pool='MAX',causal=causal_,anticausal = anticausal) \n",
    "        self.conv4 = ECoGMappingBlock_causal(base*8,base*16,[3,3,3],residual=True,resample = [(1 if self.pre_articulate else 2),2,2],pool='MAX',causal=causal_,anticausal = anticausal) \n",
    "        self.norm_mask = norm(32,base*4) \n",
    "        self.mask = ln_c.Conv3d(base*4,1,[3,1,1],1,[1,0,0] ,causal=causal_,anticausal = anticausal)\n",
    "        self.norm = norm(32,base*16)\n",
    "        self.conv5 = ln_c.Conv1d(base*16,base*16,3,1,1 ,causal=causal,anticausal = anticausal)\n",
    "        self.norm2 = norm(32,base*16)\n",
    "        if self.pre_articulate:\n",
    "            self.lin1 = ln_c.Linear(base*16,2048) # 2048 = 256*8\n",
    "        self.conv6 = Upsample_Block( 256, 128, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norm3 = norm(32,128)\n",
    "        self.conv7 = Upsample_Block( 128, 64, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norm4 = norm(32,64)\n",
    "        self.conv8 = Upsample_Block( 64, 32, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norm5 = norm(32,32)\n",
    "        self.conv9 = Upsample_Block( 32, latent_channel, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norm6 = norm(32,latent_channel)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.GR = GR\n",
    "        if GR: #gradient reversal\n",
    "            self.Speaker_Classifier_GR(Channels = [32], previous_Channels = 32, Num_Speakers = 2)\n",
    "        self.prediction_head = Speech_Para_Prediction(causal=causal, anticausal=anticausal,\\\n",
    "                                                      compute_db_loudness=compute_db_loudness,\\\n",
    "                                                      n_formants=n_formants,n_formants_noise=n_formants_noise,\\\n",
    "                                                      n_mels=n_mels,network_db=network_db,input_channel=latent_channel)\n",
    "    def forward(self,ecog,mask_prior,mni,gender='Female'):\n",
    "        #print (self.mapping_layers,single_patient_mapping,self.region_index,self.multiscale,self.pre_articulate)\n",
    "        #0 -1 0 False False\n",
    "        elec_length = int(ecog[0].shape[-1]**0.5)\n",
    "        x = ecog\n",
    "        bs_per = x.shape[0]\n",
    "        mask_prior = mask_prior\n",
    "        x = x.reshape([-1,1,x.shape[1],elec_length,elec_length])\n",
    "        x = self.from_ecog(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x[:,:,4:]\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.max(-1)[0].max(-1)[0]\n",
    "        x = self.conv5(F.leaky_relu(self.norm(x),0.2))\n",
    "        x = self.norm2(x)\n",
    "        x = self.conv6(F.leaky_relu(x,0.2))\n",
    "        x = self.conv7(F.leaky_relu(self.norm3(x),0.2))\n",
    "        x = self.conv8(F.leaky_relu(self.norm4(x),0.2))\n",
    "        x = self.conv9(F.leaky_relu(self.norm5(x),0.2))\n",
    "        x_common = F.leaky_relu(self.norm6(x),0.2)\n",
    "        #print (x_common.shape)\n",
    "        if self.GR:\n",
    "            classified_Speakers = self.Speaker_Classifier_GR(x_common)\n",
    "        components = self.prediction_head(x_common)\n",
    "        return components if not self.GR else classified_Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "batch_size = 2 * 3\n",
    "ecog = torch.randn(batch_size, 144,64).to(device).float()\n",
    "mask_prior =torch.randn(batch_size, 1,64).to(device).float()\n",
    "mni_coordinate = torch.randn(batch_size, 3,64).to(device).float()\n",
    "\n",
    "\n",
    "model_ind = 1\n",
    "ecog_encoder =  ECoGMapping_Bottleneck_ran(n_mels=32,n_formants=5,latent_channel=64).to(device)#.cuda()\n",
    "ecog_encoder.to(device)\n",
    "#ecog_encoder.to('cpu')\n",
    "#components = ecog_encoder( torch.cat(ecog,dim=0)  , torch.cat(mask_prior,dim=0), mni_coordinate )\n",
    "components = ecog_encoder(  ecog, mask_prior , mni_coordinate )\n",
    "\n",
    "print (components['freq_formants_hamon_hz'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECoGMappingRNN_ran(torch.nn.Module):\n",
    "    \"\"\" Model used to predict motion traces and speech features from ECoG data using Transformer\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_electrodes=64,\n",
    "            n_input_features=64,\n",
    "            n_classes=64, #latent feature dimension\n",
    "            n_layers=3,\n",
    "            n_rnn_units=128,\n",
    "            max_sequence_length=500,\n",
    "            bidirectional=False,\n",
    "            n_output_features=256,\n",
    "            batch_first=True,\n",
    "            dropout=0.5,\n",
    "            use_final_linear_layer=False,\n",
    "            normalize_inputs=False, \n",
    "            out_channels=None, n_mels = 32, network_db= False,\n",
    "            causal=False, n_formants = 5, n_formants_noise =1,\n",
    "            anticausal = False, compute_db_loudness=False,pre_articulate=False,GR = False):\n",
    "        super(ECoGMappingRNN_ran, self).__init__()\n",
    "        \n",
    "        # params\n",
    "        self.n_classes = n_classes\n",
    "        self.num_electrodes = n_electrodes\n",
    "        self.normalize_inputs = normalize_inputs\n",
    "\n",
    "        # rnn model\n",
    "        self.base_model = BasicRNN(\n",
    "            n_layers=n_layers,\n",
    "            n_rnn_units=n_rnn_units,\n",
    "            max_sequence_length=max_sequence_length,\n",
    "            bidirectional=False if causal else True,\n",
    "            n_input_features=n_input_features,\n",
    "            n_output_features=n_output_features,\n",
    "            batch_first=batch_first,\n",
    "            dropout=dropout,\n",
    "            use_final_linear_layer=use_final_linear_layer\n",
    "        )\n",
    "\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        latent_channel = self.n_classes\n",
    "        self.motion_projection = torch.nn.Linear(n_rnn_units * 2 if not causal else n_rnn_units, latent_channel)\n",
    "        norm = GroupNormXDim\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.GR = GR\n",
    "        if GR: #gradient reversal\n",
    "            self.Speaker_Classifier_GR(Channels = [self.n_classes], previous_Channels = self.n_classes, Num_Speakers = 2)\n",
    "        self.prediction_head = Speech_Para_Prediction(causal=causal, anticausal=anticausal,\\\n",
    "                                                      compute_db_loudness=compute_db_loudness,\\\n",
    "                                                      n_formants=n_formants,n_formants_noise=n_formants_noise,\\\n",
    "                                                      n_mels=n_mels,network_db=network_db,input_channel=latent_channel)\n",
    "    def forward(self, inputs,mask_prior,mni,gender='Female',single_patient_mapping=-1,states=None, return_states=False, **kwargs):\n",
    "\n",
    "        dim = inputs.shape[2]\n",
    "        n_features = dim // self.num_electrodes\n",
    "        if self.normalize_inputs:\n",
    "            for n in range(n_features):\n",
    "                normalized_inputs = torch.nn.functional.normalize(\n",
    "                    inputs[:, :, n * self.num_electrodes:(n + 1) * self.num_electrodes],\n",
    "                    dim=-1)\n",
    "                inputs[:, :, n * self.num_electrodes:(n + 1) * self.num_electrodes] = normalized_inputs\n",
    "        latent_representation = self.base_model.forward(inputs)\n",
    "        print ('latent_representation',latent_representation.shape)\n",
    "        x_common = self.motion_projection(self.relu(latent_representation))[:,8:-8].permute(0,2,1)\n",
    "        print ('x_common',x_common.shape)\n",
    "        if self.GR:\n",
    "            classified_Speakers = self.Speaker_Classifier_GR(x_common)\n",
    "        components = self.prediction_head(x_common)\n",
    "        return components if not self.GR else classified_Speakers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_representation torch.Size([6, 144, 256])\n",
      "x_common torch.Size([6, 128, 128])\n",
      "torch.Size([6, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "batch_size = 2 * 3\n",
    "ecog = torch.randn(batch_size, 144,64).to(device).float()\n",
    "mask_prior =torch.randn(batch_size, 1,64).to(device).float()\n",
    "mni_coordinate = torch.randn(batch_size, 3,64).to(device).float()\n",
    "\n",
    "\n",
    "model_ind = 1\n",
    "ecog_encoder =  ECoGMappingRNN_ran(n_mels=32,n_formants=5,n_classes=128).to(device)#.cuda()\n",
    "ecog_encoder.to(device)\n",
    "#ecog_encoder.to('cpu')\n",
    "#components = ecog_encoder( torch.cat(ecog,dim=0)  , torch.cat(mask_prior,dim=0), mni_coordinate )\n",
    "components = ecog_encoder(  ecog, mask_prior , mni_coordinate )\n",
    "\n",
    "print (components['freq_formants_hamon_hz'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECoGMapping_3D_SWIN(nn.Module):\n",
    "    def __init__(self,n_mels,n_formants,n_formants_noise=1,compute_db_loudness=False,causal=False, \\\n",
    "                     anticausal = False,pre_articulate=False,\\\n",
    "                     hidden_dim=256, upsample='bilinear',\\\n",
    "                     select_ind = 0, network_db=False,adim= 64,\n",
    "                      GR=0,\n",
    "                     conv_method='both',classic_attention=1,mapping_layers=0,single_patient_mapping=-1,region_index=0):\n",
    "        super(ECoGMapping_3D_SWIN, self).__init__()\n",
    "        self.causal = causal\n",
    "        self.anticausal = anticausal\n",
    "        causal_ = [1,0,0] if causal else False\n",
    "        self.n_formants = n_formants\n",
    "        self.n_mels = n_mels\n",
    "        self.n_formants_noise = n_formants_noise\n",
    "        self.compute_db_loudness = compute_db_loudness\n",
    "        self.pre_articulate = pre_articulate\n",
    "        \n",
    "        self.norm6 = nn.GroupNorm(hidden_dim, hidden_dim)\n",
    "        self.conv_fundementals = ln_c.Conv1d(hidden_dim,32,3,1,1,causal=causal,anticausal = anticausal)\n",
    "        self.norm_fundementals = nn.GroupNorm(32,32)\n",
    "        self.f0_drop = nn.Dropout()\n",
    "        self.conv_f0 = ln_c.Conv1d(32,1,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_amplitudes = ln_c.Conv1d(hidden_dim,2,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_amplitudes_h = ln_c.Conv1d(hidden_dim,2,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        if compute_db_loudness:\n",
    "            self.conv_loudness = ln_c.Conv1d(hidden_dim,1,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        else:\n",
    "            self.conv_loudness = ln_c.Conv1d(hidden_dim,1,1,1,0,bias_initial=-9.,causal=causal,anticausal = anticausal)\n",
    "\n",
    "        self.conv_formants = ln_c.Conv1d(hidden_dim,32,3,1,1,causal=causal,anticausal = anticausal)\n",
    "        self.norm_formants = nn.GroupNorm(32,32)\n",
    "        self.conv_formants_freqs = ln_c.Conv1d(32,n_formants,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_bandwidth = ln_c.Conv1d(32,n_formants,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_amplitude = ln_c.Conv1d(32,n_formants,1,1,0,causal=causal,anticausal = anticausal)\n",
    "\n",
    "        self.conv_formants_freqs_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_bandwidth_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0,causal=causal,anticausal = anticausal)\n",
    "        self.conv_formants_amplitude_noise = ln_c.Conv1d(32,n_formants_noise,1,1,0,causal=causal,anticausal = anticausal)\n",
    "\n",
    "        depthss = [ [2,2,6,2],[2,2,6,2] ]\n",
    "        patch_size_Ts = [  2,2 ]\n",
    "        patch_size_Es = [ (1,1),(2,2) ]\n",
    "        window_size_Ts = [4,8,16 ]\n",
    "        window_size_Es = [ (4,4),(4,4),(4,4) ]\n",
    "        numclassess = [ 384,384,384 ] #according to final temporal length\n",
    "        \n",
    "        if causal:\n",
    "            norm = GroupNormXDim\n",
    "        else:\n",
    "            norm = GroupNormXDim\n",
    "        base_swin = 48\n",
    "        self.swin_transformer =  SwinTransformer3D1(patch_size=(2,2,2),\n",
    "                 in_chans=1,\n",
    "                 embed_dim=48,\n",
    "                 depths=[2, 2, 6, 2],\n",
    "                 num_heads=[3, 6, 12, 24],\n",
    "                 window_size=(window_size_Ts[select_ind],2,2),\n",
    "                  #drop_path_rate=0,\n",
    "                 mlp_ratio=4.,causal=causal,anticausal=anticausal)\n",
    "        self.swin_down_times = 4 \n",
    "        self.conv6 = ln.ConvTranspose1d(numclassess[select_ind], 256, 3, 2, 1, transform_kernel=True)#,causal=causal,anticausal = anticausal)\n",
    "        \n",
    "        self.upconvs1 = Upsample_Block( 256, hidden_dim, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norms1 = norm(32,hidden_dim)\n",
    "        self.upconvs2 = Upsample_Block( hidden_dim, hidden_dim, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norms2 = norm(32,hidden_dim)\n",
    "        self.upconvs3 = Upsample_Block( hidden_dim, hidden_dim, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norms3 = norm(32,hidden_dim)\n",
    "        self.upconvs4 = Upsample_Block( hidden_dim, hidden_dim, bilinear=True if upsample=='bilinear' else False)\n",
    "        self.norms4 = norm(32,hidden_dim)\n",
    "        self.upconv_layers = [self.upconvs1,self.upconvs2,self.upconvs3,self.upconvs4]\n",
    "        self.norm_layers = [self.norms1,self.norms2,self.norms3,self.norms4]\n",
    "        self.GR = GR\n",
    "        if GR: #gradient reversal\n",
    "            self.Speaker_Classifier_GR(Channels = [self.n_classes], previous_Channels = self.n_classes, Num_Speakers = 2)\n",
    "        \n",
    "        self.prediction_head = Speech_Para_Prediction(causal=causal, anticausal=anticausal,\\\n",
    "                                                      compute_db_loudness=compute_db_loudness,\\\n",
    "                                                      n_formants=n_formants,n_formants_noise=n_formants_noise,\\\n",
    "                                                      n_mels=n_mels,network_db=network_db,input_channel=hidden_dim)\n",
    "    def forward(self,ecog,mask_prior,mni,gender='Female'):\n",
    "        elec_length = int(ecog[0].shape[-1]**0.5)\n",
    "        x = ecog[:,16:]\n",
    "        x = x.reshape([-1,x.shape[1],elec_length,elec_length,1])\n",
    "        xs = F.pad(input=x, pad=(0,0,1,0,1,0,0,0), mode='constant', value=0).permute(0,4,1,2,3) # pad to even H and W\n",
    "        bs_per = x.shape[0]\n",
    "        xs = self.swin_transformer(xs,region_index=None).squeeze(-1).squeeze(-1).transpose(1, 2)\n",
    "        x = self.conv6(F.leaky_relu(xs.transpose(1, 2),0.2))\n",
    "        \n",
    "        for i in range(self.swin_down_times-1):\n",
    "            x = self.upconv_layers[i](F.leaky_relu(self.norm_layers[i](x),0.2))\n",
    "        x_common = F.leaky_relu(self.norm6(x),0.2)\n",
    "        print ('x_common',x_common.shape)\n",
    "        if self.GR:\n",
    "            classified_Speakers = self.Speaker_Classifier_GR(x_common)\n",
    "        components = self.prediction_head(x_common)\n",
    "        return components if not self.GR else classified_Speakers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "swin transformer causal: False\n",
      "x_common torch.Size([6, 256, 128])\n",
      "torch.Size([6, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "batch_size = 2 * 3\n",
    "ecog = torch.randn(batch_size, 144,64).to(device).float()\n",
    "mask_prior =torch.randn(batch_size, 1,64).to(device).float()\n",
    "mni_coordinate = torch.randn(batch_size, 3,64).to(device).float()\n",
    "\n",
    "\n",
    "model_ind = 1\n",
    "ecog_encoder =  ECoGMapping_3D_SWIN(n_mels=32,n_formants=5 ).to(device)#.cuda()\n",
    "ecog_encoder.to(device)\n",
    "#ecog_encoder.to('cpu')\n",
    "#components = ecog_encoder( torch.cat(ecog,dim=0)  , torch.cat(mask_prior,dim=0), mni_coordinate )\n",
    "components = ecog_encoder(  ecog, mask_prior , mni_coordinate )\n",
    "\n",
    "print (components['freq_formants_hamon_hz'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
