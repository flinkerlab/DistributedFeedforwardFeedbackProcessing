{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set()\n",
    "#sns.set_style('whitegrid')\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import shutil\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "with open('/scratch/xc1490/home/projects/ecog/ALAE_1023/AllSubjectInfo.json','r') as rfile:\n",
    "    allsubj_param = json.load(rfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/xc1490/projects/ecog/ALAE_1023\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/xc1490/projects/ecog/ALAE_1023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('AllSubjectInfo.json','r') as rfile:\n",
    "    allsubj_param = json.load(rfile)\n",
    "total_samples = np.sort([sample for sample in allsubj_param['Subj'].keys() if \\\n",
    "                          allsubj_param['Subj'][sample]['Density'] == 'LD' and 'female' not in sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_LD_43 = np.array(['NY863','NY704', 'NY708', 'NY718', 'NY720', 'NY721', 'NY722', 'NY723',\n",
    "       'NY733', 'NY737', 'NY739', 'NY741', 'NY743', 'NY744', 'NY746',\n",
    "       'NY748', 'NY751', 'NY753', 'NY756', 'NY757', 'NY758', 'NY763',\n",
    "       'NY781', 'NY782', 'NY786', 'NY787', 'NY789', 'NY791', 'NY794',\n",
    "       'NY797', 'NY799', 'NY802', 'NY810', 'NY828', 'NY830', 'NY836',\n",
    "       'NY837', 'NY838', 'NY841', 'NY844', 'NY847', 'NY857', \n",
    "       'NY869','NY717', 'NY742', 'NY749','NY798','NY829'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_path(subject):\n",
    "    #print (subject)\n",
    "    nsample = 20 if subject =='NY829' else 80\n",
    "    path = '/scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_{}_nsample_{}_nfft_{}_noisedb_-50_density_{}_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/'.format(\n",
    "                           subject,nsample, 256 if allsubj_param['Subj'][subject]['Gender'] =='Female' else 512,allsubj_param['Subj'][subject]['Density'])\n",
    "    if subject=='NY863':\n",
    "        path = '/scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY863_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/'\n",
    "    if allsubj_param['Subj'][subject]['Density'] =='HB':\n",
    "        path = '/scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10140800_a2a_corpus_sub_{}_nsample_{}_nfft_{}_noisedb_-50_density_{}_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/'.format(\n",
    "                           subject,nsample, 256 if allsubj_param['Subj'][subject]['Gender'] =='Female' else 512,allsubj_param['Subj'][subject]['Density'])\n",
    "\n",
    "    #print ([i for i in os.listdir(path) if i.endswith('.pth')][0])\n",
    "    model = [i for i in os.listdir(path) if i.endswith('.pth')][0]\n",
    "    load_path = path + model\n",
    "    #print (os.path.getsize(load_path))\n",
    "    return load_path\n",
    "#for subject in  old_LD_43:\n",
    "#    print (get_model_path(subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sbatch_ecog_a2a(output_dir='',subject='NY742',density='LD',wavebased=1, bgnoisefromdata=0,\\\n",
    "                             ignore_loading=1,finetune=0,learnedmask=0, dynamicfiltershape=0,\\\n",
    "                             formant_supervision = 0,noisedb=-50,n_filter_samples=20, n_fft=256,reverse_order=1,\\\n",
    "                             pitch_supervision=0,\\\n",
    "                             lar_cap=0,intensity_thres = -1,unified=0,use_stoi = 0,use_denoise = 0,inference=0,\\\n",
    "                             pretrained_model_dir='None',epoch_num=1 ):\n",
    "    text = '#!/bin/bash'\n",
    "    text += '\\n'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --job-name=aLD{}'.format(subject[2:])\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --nodes=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --cpus-per-task=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --mem=36GB'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --time=36:00:00'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --gres=gpu:1'\n",
    "    text += '\\n'\n",
    "    text += 'overlay_ext3=/home/xc1490/home/apps/ddsp.ext3'\n",
    "    text +='\\n'\n",
    "    text +='singularity exec --nv \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    --overlay ${overlay_ext3}:ro \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /bin/bash -c \"'\n",
    "    text +='\\n'\n",
    "    text +='source /ext3/env.sh'\n",
    "    text +='\\n'\n",
    "    text +='cd /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/'\n",
    "    text +='\\n'\n",
    "    text +='python train_a2a_infer.py --OUTPUT_DIR {}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_{}_dynamic_{}  --subject {} --DENSITY {} --noise_db {} --formant_supervision {} --wavebased {} --bgnoise_fromdata {} --ignore_loading {} --finetune {} --learnedmask {} --dynamicfiltershape {} --n_filter_samples {} --n_fft {} --reverse_order {} --lar_cap {} --intensity_thres {} --pitch_supervision {} --unified {} --use_stoi {} --use_denoise {} --pretrained_model_dir {} --epoch_num {}\"'.format(\\\n",
    "                            output_dir,subject,n_filter_samples,use_denoise, use_stoi, n_fft, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask, reverse_order,dynamicfiltershape,subject, density, noisedb, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune,learnedmask, dynamicfiltershape,n_filter_samples, n_fft, reverse_order,lar_cap,intensity_thres ,pitch_supervision,unified,use_stoi,use_denoise, pretrained_model_dir, epoch_num)\n",
    "    text +='\\n'\n",
    "    \n",
    "    job_file = 'jobs/a2a_sub_{}_nsample_{}_use_denoise_{}stoi_{}_nfft_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_{}_noise{}_reverse_{}_dynamic_{}.sbatch'.format(subject,n_filter_samples, use_denoise,use_stoi,n_fft, density, formant_supervision, wavebased,bgnoisefromdata, ignore_loading,finetune,learnedmask,output_dir.split('/')[-1],noisedb, reverse_order,dynamicfiltershape)\n",
    "    #print (text)\n",
    "    #os.remove(job_file)\n",
    "    #if not os.path.exists(job_file):\n",
    "    #    os.makedirs(job_file)\n",
    "    f= open(job_file,\"w+\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    print ('sbatch ',job_file)\n",
    "    return text.split('\\n')[-2][:-1] + '\\n'#'output/{}_a2a_sub_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}'.format(\\\n",
    "            #                output_dir.split('/')[-1],subject, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch  jobs/a2a_sub_NY863_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY704_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY708_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY718_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY720_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY721_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY722_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY723_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY733_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY737_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY739_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY741_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY743_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY744_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY746_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY748_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY751_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY753_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY756_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY757_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY758_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY763_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY781_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY782_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY786_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY787_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY789_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY791_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY794_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY797_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY799_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY802_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY810_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY828_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY830_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY836_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY837_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY838_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY841_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY844_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY847_nsample_80_use_denoise_0stoi_0_nfft_256_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY857_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY869_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY717_nsample_80_use_denoise_0stoi_0_nfft_256_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY742_nsample_80_use_denoise_0stoi_0_nfft_256_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY749_nsample_80_use_denoise_0stoi_0_nfft_256_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY798_nsample_80_use_denoise_0stoi_0_nfft_512_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n",
      "sbatch  jobs/a2a_sub_NY829_nsample_20_use_denoise_0stoi_0_nfft_512_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "wavebased = 1\n",
    "use_denoise = 0\n",
    "stoi = 0\n",
    "texts = ''\n",
    "for noisedb in [ -50]:  #-40???\n",
    "    for bgnoisefromdata in [1 ]:  #0????\n",
    "            for subject in  old_LD_43 :\n",
    "                #print (subject)\n",
    "                formant_supervision = 0\n",
    "                #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                for learnedmask  in [1]:\n",
    "                    if allsubj_param['Subj'][subject]['Gender'] =='Female':\n",
    "                        n_ffts = [256]\n",
    "                    else:\n",
    "                        n_ffts = [512]\n",
    "                    for n_fft in n_ffts:\n",
    "\n",
    "                        #if n_fft == 256:\n",
    "                        #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                                load_sub_dir = get_model_path(subject)#'output/a2a_05140800_a2a_corpus_sub_{}_nsample_80_nfft_{}_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0'.format( subject,n_fft )\n",
    "                        #if os.path.exists( load_sub_dir):\n",
    "                            #if len(os.listdir(load_sub_dir)) >= 10:\n",
    "                                for n_filter_samples in [ 80 ]: #150????\n",
    "                                    for dynamicfiltershape in [0 ]:#1???\n",
    "                                        if subject =='NY829':\n",
    "                                            n_filter_samples = 20\n",
    "                                        ignore_loading = 0\n",
    "                                        finetune = 1\n",
    "                                        density = allsubj_param['Subj'][subject]['Density']\n",
    "                                        pitch_supervision = 0\n",
    "                                        output_dir = 'output_new/a2a_0807/INFER_a2a_10050800'\n",
    "                                        outpath = 'output_new/a2a_0807/INFER_{}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_1_dynamic_0/'.format(\\\n",
    "                                                    output_dir.split('/')[-1],subject, n_filter_samples,use_denoise,stoi, n_fft,noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n",
    "                                        #a2a_10050800_a2a_corpus_sub_NY688_nsample_80_nfft_256_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/\n",
    "                                        if not os.path.exists(outpath):\n",
    "                                            texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune,use_denoise=use_denoise, \\\n",
    "                                                                     formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,\\\n",
    "                                                                     n_filter_samples= n_filter_samples,\\\n",
    "                                                                     pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                            count += 1\n",
    "                                        else:\n",
    "                                            epochs = [file.split('.')[0].split('epoch')[-1] for file in os.listdir(outpath) if file.endswith('pth')]\n",
    "                                            if len(epochs) == 0:\n",
    "                                            #if maxepoch:\n",
    "                                                texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, \\\n",
    "                                                                        use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,\\\n",
    "                                                                         learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,\\\n",
    "                                                                         pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                                count += 1\n",
    "                                            else:\n",
    "                                                #generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape)\n",
    "                                                count += 1\n",
    "                                                if np.max(np.array(epochs).astype('int')) >90:\n",
    "                                                    print ('\\n')\n",
    "                                                    #print ('enough training, ', subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/xc1490/projects/ecog/ALAE_1023'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "#SBATCH --job-name=aLD829\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --cpus-per-task=1\r\n",
      "#SBATCH --mem=36GB\r\n",
      "#SBATCH --time=36:00:00\r\n",
      "#SBATCH --gres=gpu:1\r\n",
      "overlay_ext3=/home/xc1490/home/apps/ddsp.ext3\r\n",
      "singularity exec --nv \\\r\n",
      "    --overlay ${overlay_ext3}:ro \\\r\n",
      "    /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \\\r\n",
      "    /bin/bash -c \"\r\n",
      "source /ext3/env.sh\r\n",
      "cd /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/\r\n",
      "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0807/INFER_a2a_10050800_a2a_corpus_sub_NY829_nsample_20_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0  --subject NY829 --DENSITY HB --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 20 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10140800_a2a_corpus_sub_NY829_nsample_20_nfft_512_noisedb_-50_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1\"\r\n"
     ]
    }
   ],
   "source": [
    "cat jobs/a2a_sub_NY829_nsample_20_use_denoise_0stoi_0_nfft_512_density_HB_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_INFER_a2a_10050800_noise-50_reverse_1_dynamic_0.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "keys = ['f0', 'f0_hz', 'loudness', 'amplitudes', 'amplitudes_h', 'freq_formants_hamon', 'bandwidth_formants_hamon',\\\n",
    " 'freq_formants_hamon_hz', 'bandwidth_formants_hamon_hz', 'amplitude_formants_hamon', 'freq_formants_noise', \\\n",
    " 'bandwidth_formants_noise', \\\n",
    " 'freq_formants_noise_hz', 'bandwidth_formants_noise_hz', 'amplitude_formants_noise', 'noisebg' ]\n",
    "other_keys = ['on_stage', 'wave_orig', 'spec', 'labels']\n",
    "for subject in tqdm(old_LD_43):\n",
    "    print (subject)\n",
    "    if not os.path.exists('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/merge/{}/train.npy'.format(subject)):\n",
    "        try:\n",
    "            train_file = np.sort(np.array([i.split('.npy')[0].split('_')[-1] for i in \\\n",
    "             os.listdir('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/{}'.format(subject)) \\\n",
    "                     if i.startswith('train')]).astype('int'))\n",
    "            store_all = {}\n",
    "            for key in keys:\n",
    "                store_all[key] = []\n",
    "            for key in other_keys:\n",
    "                store_all[key] = []\n",
    "\n",
    "            for i in tqdm(train_file):\n",
    "                componenets = np.load('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/{}/train_{}.npy'.format(subject,i),allow_pickle=True\\\n",
    "                       ).item()\n",
    "                for key in keys:\n",
    "                    #print (componenets[key].shape)\n",
    "                    store_all[key].append(componenets[key])\n",
    "                for key in other_keys:\n",
    "                    #print (componenets['others'][key].shape)\n",
    "                    store_all[key].append(componenets['others'][key])\n",
    "                #print (componenets['others']['labels'])\n",
    "            for key in np.concatenate((keys,other_keys)):\n",
    "                store_all[key] = np.concatenate((store_all[key]))\n",
    "            os.makedirs('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/merge/{}/'.format(subject))\n",
    "            np.save('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/merge/{}/train'.format(subject),store_all)\n",
    "            shutil.copyfile('/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/{}/test.npy'.format(subject),\\\n",
    "                            '/scratch/xc1490/ECoG_Shared_Data/formant_syn/componets/merge/{}/test.npy'.format(subject)\n",
    "                           )\n",
    "        except:\n",
    "            print (subject,'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer a2a quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/xc1490/projects/ecog/ALAE_1023\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/xc1490/projects/ecog/ALAE_1023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sbatch_ecog_a2a(output_dir='',subject='NY742',density='LD',wavebased=1, bgnoisefromdata=0,\\\n",
    "                             ignore_loading=1,finetune=0,learnedmask=0, dynamicfiltershape=0,\\\n",
    "                             formant_supervision = 0,noisedb=-50,n_filter_samples=20, n_fft=256,reverse_order=1,\\\n",
    "                             pitch_supervision=0,\\\n",
    "                             lar_cap=0,intensity_thres = -1,unified=0,use_stoi = 0,use_denoise = 0,inference=0,\\\n",
    "                             pretrained_model_dir='None',epoch_num=1,quantfilename='',quantind=0 ):\n",
    "    text = '#!/bin/bash'\n",
    "    text += '\\n'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --job-name=aLD{}'.format(subject[2:])\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --nodes=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --cpus-per-task=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --mem=36GB'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --time=00:10:00'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --gres=gpu:1'\n",
    "    text += '\\n'\n",
    "    text += 'overlay_ext3=/home/xc1490/home/apps/ddsp.ext3'\n",
    "    text +='\\n'\n",
    "    text +='singularity exec --nv \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    --overlay ${overlay_ext3}:ro \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /bin/bash -c \"'\n",
    "    text +='\\n'\n",
    "    text +='source /ext3/env.sh'\n",
    "    text +='\\n'\n",
    "    text +='cd /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/'\n",
    "    text +='\\n'\n",
    "    text +='python train_a2a_infer.py --OUTPUT_DIR {}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_{}_dynamic_{}_quantind_{}  --subject {} --DENSITY {} --noise_db {} --formant_supervision {} --wavebased {} --bgnoise_fromdata {} --ignore_loading {} --finetune {} --learnedmask {} --dynamicfiltershape {} --n_filter_samples {} --n_fft {} --reverse_order {} --lar_cap {} --intensity_thres {} --pitch_supervision {} --unified {} --use_stoi {} --use_denoise {} --pretrained_model_dir {} --epoch_num {} --quantfilename {}\"'.format(\\\n",
    "                            output_dir,subject,n_filter_samples,use_denoise, use_stoi, n_fft, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask, reverse_order,\\\n",
    "                            dynamicfiltershape,quantind,subject, density, noisedb, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,\\\n",
    "                            finetune,learnedmask, dynamicfiltershape,n_filter_samples, n_fft, reverse_order,lar_cap,intensity_thres ,\\\n",
    "                            pitch_supervision,unified,use_stoi,use_denoise, pretrained_model_dir, epoch_num,quantfilename)\n",
    "    text +='\\n'\n",
    "    \n",
    "    job_file = 'jobs/a2a_sub_{}_nsample_{}_use_denoise_{}stoi_{}_nfft_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_{}_noise{}_reverse_{}_dynamic_{}_{}.sbatch'.format(subject,n_filter_samples, use_denoise,use_stoi,n_fft, density, \\\n",
    "    formant_supervision, wavebased,bgnoisefromdata, ignore_loading,finetune,learnedmask,output_dir.split('/')[-1],\\\n",
    "    noisedb, reverse_order,dynamicfiltershape,quantind)\n",
    "    #print (text)\n",
    "    #os.remove(job_file)\n",
    "    #if not os.path.exists(job_file):\n",
    "    #    os.makedirs(job_file)\n",
    "    f= open(job_file,\"w+\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    outdir = '{}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_{}_dynamic_{}_quantind_{}'.format(output_dir,subject,n_filter_samples,use_denoise, use_stoi, n_fft, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask, reverse_order,\\\n",
    "                            dynamicfiltershape,quantind)\n",
    "    #print (outdir)\n",
    "    if not os.path.exists(outdir):\n",
    "        print ('sbatch ',job_file)\n",
    "        return text.split('\\n')[-2][:-1] + '\\n'\n",
    "        #'output/{}_a2a_sub_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}'.format(\\\n",
    "                #                output_dir.split('/')[-1],subject, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n",
    "    else:\n",
    "        if len(os.listdir(outdir))>1:\n",
    "            #print ('exists!')\n",
    "            print ('sbatch ',job_file)\n",
    "            return ''\n",
    "        else:\n",
    "            print ('sbatch ',job_file)\n",
    "            return text.split('\\n')[-2][:-1] + '\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/all/'\n",
    "quantifiles = ['{}_kmeans_model_Cluster{}_Window{}_6formants.pkl'.format(sample,cluster,window) for sample in old_LD_43\n",
    "    for cluster in [100,200,500,1000] for window in [1,2,3]]\n",
    "quantifiles = [root_dir+i for i in quantifiles]\n",
    "#root_dir = '/scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/multisubjectmodel/'\n",
    "#quantifiles2 = np.sort(os.listdir(root_dir))\n",
    "#quantifiles2 = [root_dir+i for i in quantifiles2]\n",
    "#quantifiles = np.concatenate((quantifiles, quantifiles2)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[os.path.exists(file) for file in quantifiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "wavebased = 1\n",
    "use_denoise = 0\n",
    "stoi = 0\n",
    "texts = ''\n",
    "for noisedb in [ -50]:  #-40???\n",
    "    for bgnoisefromdata in [1 ]:  #0????\n",
    "            for subject in   old_LD_43 :#['NY869']:#old_LD_43 :\n",
    "                #print (subject)\n",
    "                formant_supervision = 0\n",
    "                #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                for learnedmask  in [1]:\n",
    "                    if allsubj_param['Subj'][subject]['Gender'] =='Female':\n",
    "                        n_ffts = [256]\n",
    "                    else:\n",
    "                        n_ffts = [512]\n",
    "                    for n_fft in n_ffts:\n",
    "\n",
    "                        #if n_fft == 256:\n",
    "                        #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                                load_sub_dir = get_model_path(subject)#'output/a2a_05140800_a2a_corpus_sub_{}_nsample_80_nfft_{}_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0'.format( subject,n_fft )\n",
    "                        #if os.path.exists( load_sub_dir):\n",
    "                            #if len(os.listdir(load_sub_dir)) >= 10:\n",
    "                                for n_filter_samples in [ 80 ]: #150????\n",
    "                                    for dynamicfiltershape in [0 ]:#1???\n",
    "                                        if subject =='NY829':\n",
    "                                            n_filter_samples = 20\n",
    "                                        ignore_loading = 0\n",
    "                                        finetune = 1\n",
    "                                        density = allsubj_param['Subj'][subject]['Density']\n",
    "                                        pitch_supervision = 0\n",
    "                                        output_dir = 'output_new/a2a_0814/quantization_INFER_a2a_08140800'\n",
    "                                        outpath = 'output_new/a2a_0814/quantization_INFER_{}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_1_dynamic_0/'.format(\\\n",
    "                                                    output_dir.split('/')[-1],subject, n_filter_samples,use_denoise,stoi, n_fft,noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n",
    "                                        #a2a_10050800_a2a_corpus_sub_NY688_nsample_80_nfft_256_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/\n",
    "                                        quantifiles = ['{}_kmeans_model_Cluster{}_Window{}_6formants.pkl'.format(subject,cluster,window)  \n",
    "                                            for cluster in [100,200,500,1000] for window in [1,2,3]]\n",
    "                                        quantifiles = [root_dir+i for i in quantifiles]\n",
    "                                        \n",
    "                                        for quantind in range(len(quantifiles)):\n",
    "                                            quantfilename = quantifiles[quantind]\n",
    "                                            if not os.path.exists(outpath):\n",
    "                                                texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune,use_denoise=use_denoise, \\\n",
    "                                                                         formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,\\\n",
    "                                                                         n_filter_samples= n_filter_samples,\\\n",
    "                                                                         pitch_supervision=pitch_supervision,quantfilename=quantfilename,quantind=quantind,\\\n",
    "                                                                                  dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                                texts+='\\n'\n",
    "                                                count += 1\n",
    "                                            else:\n",
    "                                                epochs = [file.split('.')[0].split('epoch')[-1] for file in os.listdir(outpath) if file.endswith('pth')]\n",
    "                                                if len(epochs) == 0:\n",
    "                                                #if maxepoch:\n",
    "                                                    texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, \\\n",
    "                                                                            use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,\\\n",
    "                                                                             learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,quantfilename=quantfilename,quantind=quantind,\\\n",
    "                                                                             pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                                    texts+='\\n'\n",
    "                                                    count += 1\n",
    "                                                else:\n",
    "                                                    #generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape)\n",
    "                                                    count += 1\n",
    "                                                    if np.max(np.array(epochs).astype('int')) >90:\n",
    "                                                        print ('\\n')\n",
    "                                                        #print ('enough training, ', subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_1  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/869model/NY869_kmeans_model_Cluster100_Window3_6formants.pkl\n",
    "\n",
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_3  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/869model/NY869_kmeans_model_Cluster200_Window3_6formants.pkl\n",
    "\n",
    "\n",
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_6  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/multisubjectmodel/kmeans_model_Cluster1000_Window3_6formants.pkl\n",
    "\n",
    "\n",
    "\n",
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_8  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/multisubjectmodel/kmeans_model_Cluster100_Window3_6formants.pkl\n",
    "\n",
    "\n",
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_10  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/multisubjectmodel/kmeans_model_Cluster200_Window3_6formants.pkl\n",
    "\n",
    "\n",
    "python train_a2a_infer.py --OUTPUT_DIR output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_12  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 1 --quantfilename /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/data/models/multisubjectmodel/kmeans_model_Cluster500_Window3_6formants.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_log_test.csv  \u001b[0m\u001b[38;5;13msample_1_NY869.png\u001b[0m      sample_1_NY869.png_sample_npy_1.npy\r\n",
      "\u001b[38;5;13mresult_1_NY869.png\u001b[0m  \u001b[38;5;45msample_1_NY869.png.wav\u001b[0m  \u001b[38;5;45msample_1_NY869.pngdenoise.wav\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls output_new/a2a_0813/quantization_INFER_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0_quantind_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/xc1490/projects/ecog/ALAE_1023'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sbatch_ecog_a2a(output_dir='',subject='NY742',density='LD',wavebased=1, bgnoisefromdata=0,\\\n",
    "                             ignore_loading=1,finetune=0,learnedmask=0, dynamicfiltershape=0,\\\n",
    "                             formant_supervision = 0,noisedb=-50,n_filter_samples=20, n_fft=256,reverse_order=1,\\\n",
    "                             pitch_supervision=0,\\\n",
    "                             lar_cap=0,intensity_thres = -1,unified=0,use_stoi = 0,use_denoise = 0,inference=0,\\\n",
    "                             pretrained_model_dir='None',epoch_num=100 ):\n",
    "    text = '#!/bin/bash'\n",
    "    text += '\\n'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --job-name=aLD{}'.format(subject[2:])\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --nodes=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --cpus-per-task=1'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --mem=36GB'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --time=36:00:00'\n",
    "    text += '\\n'\n",
    "    text += '#SBATCH --gres=gpu:1'\n",
    "    text += '\\n'\n",
    "    text += 'overlay_ext3=/home/xc1490/home/apps/ddsp.ext3'\n",
    "    text +='\\n'\n",
    "    text +='singularity exec --nv \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    --overlay ${overlay_ext3}:ro \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \\\\'\n",
    "    text +='\\n'\n",
    "    text +='    /bin/bash -c \"'\n",
    "    text +='\\n'\n",
    "    text +='source /ext3/env.sh'\n",
    "    text +='\\n'\n",
    "    text +='cd /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/'\n",
    "    text +='\\n'\n",
    "    text +='python train_a2a.py --OUTPUT_DIR {}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_{}_dynamic_{}  --subject {} --DENSITY {} --noise_db {} --formant_supervision {} --wavebased {} --bgnoise_fromdata {} --ignore_loading {} --finetune {} --learnedmask {} --dynamicfiltershape {} --n_filter_samples {} --n_fft {} --reverse_order {} --lar_cap {} --intensity_thres {} --pitch_supervision {} --unified {} --use_stoi {} --use_denoise {} --pretrained_model_dir {} --epoch_num {}\"'.format(\\\n",
    "                            output_dir,subject,n_filter_samples,use_denoise, use_stoi, n_fft, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask, reverse_order,dynamicfiltershape,subject, density, noisedb, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune,learnedmask, dynamicfiltershape,n_filter_samples, n_fft, reverse_order,lar_cap,intensity_thres ,pitch_supervision,unified,use_stoi,use_denoise, pretrained_model_dir, epoch_num)\n",
    "    text +='\\n'\n",
    "    \n",
    "    job_file = 'jobs/a2a_sub_{}_nsample_{}_use_denoise_{}stoi_{}_nfft_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_{}_noise{}_reverse_{}_dynamic_{}.sbatch'.format(subject,n_filter_samples, use_denoise,use_stoi,n_fft, density, formant_supervision, wavebased,bgnoisefromdata, ignore_loading,finetune,learnedmask,output_dir.split('/')[-1],noisedb, reverse_order,dynamicfiltershape)\n",
    "    #print (text)\n",
    "    #os.remove(job_file)\n",
    "    #if not os.path.exists(job_file):\n",
    "    #    os.makedirs(job_file)\n",
    "    f= open(job_file,\"w+\")\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    print ('sbatch ',job_file)\n",
    "    return text.split('\\n')[-2][:-1] + '\\n'#'output/{}_a2a_sub_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}'.format(\\\n",
    "            #                output_dir.split('/')[-1],subject, noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch  jobs/a2a_sub_NY869_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_quantization_a2a_08130800_noise-50_reverse_1_dynamic_0.sbatch\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "wavebased = 1\n",
    "use_denoise = 0\n",
    "stoi = 0\n",
    "texts = ''\n",
    "for noisedb in [ -50]:  #-40???\n",
    "    for bgnoisefromdata in [1 ]:  #0????\n",
    "            for subject in ['NY869']: #old_LD_43 :\n",
    "                #print (subject)\n",
    "                formant_supervision = 0\n",
    "                #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                for learnedmask  in [1]:\n",
    "                    if allsubj_param['Subj'][subject]['Gender'] =='Female':\n",
    "                        n_ffts = [256]\n",
    "                    else:\n",
    "                        n_ffts = [512]\n",
    "                    for n_fft in n_ffts:\n",
    "\n",
    "                        #if n_fft == 256:\n",
    "                        #if allsubj_param['Subj'][subject]['Gender'] =='Male':\n",
    "                                load_sub_dir =  get_model_path(subject)#'output/a2a_05140800_a2a_corpus_sub_{}_nsample_80_nfft_{}_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0'.format( subject,n_fft )\n",
    "                        #if os.path.exists( load_sub_dir):\n",
    "                            #if len(os.listdir(load_sub_dir)) >= 10:\n",
    "                                for n_filter_samples in [ 80 ]: #150????\n",
    "                                    for dynamicfiltershape in [0 ]:#1???\n",
    "                                        if subject =='NY829':\n",
    "                                            n_filter_samples = 20\n",
    "                                        ignore_loading = 0\n",
    "                                        finetune = 1\n",
    "                                        density = allsubj_param['Subj'][subject]['Density']\n",
    "                                        pitch_supervision = 0\n",
    "                                        output_dir = 'output_new/a2a_0813/quantization_a2a_08130800'\n",
    "                                        outpath = 'output_new/a2a_0813/quantization_{}_a2a_corpus_sub_{}_nsample_{}_use_denoise_{}_stoi_{}_nfft_{}_noisedb_{}_density_{}_formantsup_{}_wavebased_{}_bgnoisefromdata_{}_load_{}_ft_{}_learnfilter_{}_reverse_1_dynamic_0/'.format(\\\n",
    "                                                    output_dir.split('/')[-1],subject, n_filter_samples,use_denoise,stoi, n_fft,noisedb, density, formant_supervision, wavebased,bgnoisefromdata,ignore_loading,finetune, learnedmask)\n",
    "                                        #a2a_10050800_a2a_corpus_sub_NY688_nsample_80_nfft_256_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/\n",
    "                                        if not os.path.exists(outpath):\n",
    "                                            texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune,use_denoise=use_denoise, \\\n",
    "                                                                     formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,\\\n",
    "                                                                     n_filter_samples= n_filter_samples,\\\n",
    "                                                                     pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                            count += 1\n",
    "                                        else:\n",
    "                                            epochs = [file.split('.')[0].split('epoch')[-1] for file in os.listdir(outpath) if file.endswith('pth')]\n",
    "                                            if len(epochs) == 0:\n",
    "                                            #if maxepoch:\n",
    "                                                texts += generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, \\\n",
    "                                                                        use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,\\\n",
    "                                                                         learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,\\\n",
    "                                                                         pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape,pretrained_model_dir=load_sub_dir)\n",
    "                                                count += 1\n",
    "                                            else:\n",
    "                                                #generate_sbatch_ecog_a2a(output_dir,subject,density,wavebased,bgnoisefromdata,ignore_loading,finetune, use_denoise=use_denoise,formant_supervision = formant_supervision,noisedb=noisedb,learnedmask=learnedmask,n_fft=n_fft,n_filter_samples= n_filter_samples,pitch_supervision=pitch_supervision,dynamicfiltershape=dynamicfiltershape)\n",
    "                                                count += 1\n",
    "                                                if np.max(np.array(epochs).astype('int')) >90:\n",
    "                                                    print ('\\n')\n",
    "                                                    #print ('enough training, ', subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "\r\n",
      "#SBATCH --job-name=aLD869\r\n",
      "#SBATCH --nodes=1\r\n",
      "#SBATCH --cpus-per-task=1\r\n",
      "#SBATCH --mem=36GB\r\n",
      "#SBATCH --time=36:00:00\r\n",
      "#SBATCH --gres=gpu:1\r\n",
      "overlay_ext3=/home/xc1490/home/apps/ddsp.ext3\r\n",
      "singularity exec --nv \\\r\n",
      "    --overlay ${overlay_ext3}:ro \\\r\n",
      "    /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif \\\r\n",
      "    /bin/bash -c \"\r\n",
      "source /ext3/env.sh\r\n",
      "cd /scratch/xc1490/projects/ecog/ALAE_1023/neural_speech_decoding/\r\n",
      "python train_a2a.py --OUTPUT_DIR output_new/a2a_0813/quantization_a2a_08130800_a2a_corpus_sub_NY869_nsample_80_use_denoise_0_stoi_0_nfft_512_noisedb_-50_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0  --subject NY869 --DENSITY LD --noise_db -50 --formant_supervision 0 --wavebased 1 --bgnoise_fromdata 1 --ignore_loading 0 --finetune 1 --learnedmask 1 --dynamicfiltershape 0 --n_filter_samples 80 --n_fft 512 --reverse_order 1 --lar_cap 0 --intensity_thres -1 --pitch_supervision 0 --unified 0 --use_stoi 0 --use_denoise 0 --pretrained_model_dir /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth --epoch_num 100\"\r\n"
     ]
    }
   ],
   "source": [
    "cat jobs/a2a_sub_NY869_nsample_80_use_denoise_0stoi_0_nfft_512_density_LD_formantsup_0_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_quantization_a2a_08130800_noise-50_reverse_1_dynamic_0.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p output_new/a2a_0813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rwxr--+ 1 xc1490 84691608 May  1 21:56 \u001b[0m\u001b[38;5;40m/scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth\u001b[0m\u001b[K*\r\n"
     ]
    }
   ],
   "source": [
    "ll /scratch/xc1490/ECoG_Shared_Data/demo/output_new/a2a_1005/a2a_10050800_a2a_corpus_sub_NY869_nsample_80_nfft_512_noisedb_-50_density_LD_formantsup_1_wavebased_1_bgnoisefromdata_1_load_0_ft_1_learnfilter_1_reverse_1_dynamic_0/model_epoch95.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
